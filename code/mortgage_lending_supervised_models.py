# -*- coding: utf-8 -*-
"""GOVT_650/final project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uYqpBcyehJDPju8MjZZ2obFxGOMM67kS

# **Business Understanding (Problem Statement)**


1. The research question: What is impact of certain characterstics of a first time homebuyer (depending on the race) to enable mortgage delinquency payments.

2. Revised research question: What is the impact of clustering methods when uncovering distinct profiles of first-time homebuyers to predict mortgage delinquency and defaults?

  This emphasize behavioral and demographic diversity for clustering.

3. Another revised research question: What is the impact of the financial characteristics of first-time homebuyers, including race to predict mortgage delinquency rate?

  This focus on financial health predictors (e.g., credit score, debt ratios, income).

# **Data Understanding**

1. Public Use Database - Federal Home Loan Bank System
HERA Section 1212 requires the Director to make available to the public, in a form that is useful to the public (including forms accessible electronically), and to the extent practicable, census tract level data relating to mortgages purchased by each Federal Home Loan Bank. https://www.fhfa.gov/data/public-use-database-fannie-mae-and-freddie-mac

  The contents of these files are unaudited and are reported directly by the Federal Home Loan Banks to FHFA. Questions concerning the content of these files should be directed to the relevant Federal Home Loan Bank(s).

2. The FHLBS Data sets are collected from 2020 to 2022

3. Most of the characteristics investigated contain no missing values.

4. This includes all the relevenat information about the each borrower from different states and counties

**Import the Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
import geopandas as gpd
!pip install geopandas
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc

warnings.filterwarnings('ignore')

"""**Import the 2020 CSV file**"""

mortgagedata_2020 = pd.read_csv('/content/2020_PUDB_EXPORT_123120.csv')

mortgagedata_2020.info()

"""**Import the 2021 CSV file**"""

mortgagedata_2021 = pd.read_csv('/content/2021_PUDB_EXPORT_123121.csv')

mortgagedata_2021.info()

mortgagedata_2021.head()

"""**Import the 2022 CSV file**"""

mortgagedata_2022 = pd.read_csv('/content/2022_PUDB_EXPORT_123122.csv')

mortgagedata_2022.info()

mortgagedata_2022.head()

"""## Merging the datasets

**1. Combined dataset from 2020, 2021 and 2022.**

**2. Select the first-time home buyers**
- Selected ['BorrowerFirstTimeHomebuyer'] == 1. This indicates first time home buyers
- Selected ['PropertyUsageType'] == 1. This indicates first-time homeownership, as it represents the primary residence of the property owner.
"""

mortgagedata_covid = pd.concat([mortgagedata_2020, mortgagedata_2021, mortgagedata_2022], ignore_index=True)
# Assuming 'first_time_homebuyer' is the column indicating first-time buyers.
# Replace 'first_time_homebuyer' with the actual column name if different.
first_time_buyers = mortgagedata_covid[(mortgagedata_covid['BorrowerFirstTimeHomebuyer'] == 1) & (mortgagedata_covid['PropertyUsageType'] == 1)]

first_time_buyers.info()

first_time_buyers.describe()

"""#**Data Preparation**

### Elimination of Data Columns

1. Have to eliminate some columns in the list:

  - LoanCharacteristicsID
  - NoteDate
  - LoanAcquistionDate
  - Borrower1Race2Type
  - Borrower2Race3Type
  - Borrower2Race4Type
  - Borrower2Race5Type
  - PrepaymentPenaltyExpirationDate

  Therefore, there are no null values for the remaining columns in the dataset.
"""

first_time_buyersv2 = first_time_buyers.copy()
first_time_buyersv2.info()

first_time_buyersv2.drop(columns = ['LoanCharacteristicsID',
                                    'NoteDate',
                                    'Borrower1Race2Type', 'BorrowerCount',
                                    'Borrower2Race3Type', 'Borrower2Race4Type',
                                    'Borrower2Race5Type', 'PrepaymentPenaltyExpirationDate',
                                    'LoanAcquistionDate'], inplace = True)

first_time_buyers_v3 = first_time_buyersv2.copy()
first_time_buyers_v3.head()

"""**Topic 1: Impact of financial characteristics of first-time homebuyers, including race, to predict mortgage delinquency rates**

**Key Variables to Consider:**

- **TotalMonthlyIncomeAmount:** Directly reflects financial stability.
NoteAmount: Represents loan size, which could impact repayment capacity.
- **LTVRatioPercent**
- **HousingExpenseRatioPercent:** Measures housing costs relative to income; an important predictor of financial strain.
- **TotalDebtExpenseRatioPercent:** Indicates overall debt burden and risk.
- **Borrower1CreditScoreValue:** Strong indicator of borrower financial health and likelihood of delinquency.
- **PMICoveragePercent:** Mortgage insurance coverage can mitigate risk for lenders and impact delinquency likelihood.
- **Borrower1AgeatApplicationYears:** May correlate with financial maturity and experience.
- **PropertyType:** Indicates whether the property type has differing delinquency risks (e.g., single-family vs. multi-unit).
- **MortgageType:** Determines the structure of the loan, which could affect affordability and delinquency risk.
- **LoanAcquisitionActualUPBAmt**: provides insight into the loan's size and risk exposure. A higher unpaid balance could increase the financial strain on borrowers, especially for first-time homebuyers with limited resources.
- **Borrower1Race1Type**
- **Borrower1EthnicityType**
- **Borrower1GenderType**



**Topic 2: Impact of clustering methods when uncovering distinct profiles of first-time homebuyers to predict mortgage delinquency and defaults**


**Key Variables to Consider:**
- **TotalMonthlyIncomeAmount:** A core feature to differentiate buyer profiles.
- **Borrower1CreditScoreValue:** Critical for clustering based on creditworthiness.
- **LTVRatioPercent**
- **HousingExpenseRatioPercent and TotalDebtExpenseRatioPercent:** Define financial strain and affordability, key to grouping behaviors.
- **LoanPurposeType:** Could differentiate motivations (e.g., purchase vs. refinance).
- **ScheduledTotalPaymentCount:** Helps distinguish between short- and long-term borrowers.
- **LoanAmortizationMaxTermMonths:** Impacts repayment behavior over time.
- **Borrower1AgeatApplicationYears:** Relevant for creating demographic profiles.
- **EmployementBorrowerSelfEmployed:** Employment type might correlate with income stability and repayment risk.
- **ProductCategoryName:** Indicates the loan's structure and interest terms, influencing borrower profiles.
- **PropertyUnitCount:** Distinguishes between single-family homes and multi-unit investments.
- **LoanAcquisitionActualUPBAmt**: help uncover distinct borrower profiles. Segment borrowers based on their unpaid balance alongside other financial and demographic variables (e.g., income, debt ratios, and age).
Borrowers with high balances might form a cluster that shows a unique delinquency risk profile.
- **Borrower1Race1Type**
- **Borrower1EthnicityType**
- **Borrower1GenderType**

**Variables less relevant to the topics**

- **MarginRatePercent:** This variable is highly specialized and may have less direct influence on first-time homebuyer profiles or delinquency unless the loan is adjustable-rate.
- **IndexSourceType:** Could be excluded unless focusing specifically on interest rate mechanisms.
- **MortgageLoanSellerInstType:** Less relevant unless institutional lender types significantly impact borrower clustering or delinquency.

**Variables relevant to data visualisations**
-  Bank
- FIPSStateNumericCode     
- FIPSCountyCode                       
- CensusTractIdentifier           
- CensusTractMinorityRatioPercent   
- CensusTractMedFamIncomeAmount    
- LocalAreaMedianIncomeAmount

"""

fin_char_data = first_time_buyers_v3[['Bank', 'FIPSStateNumericCode',
                                      'FIPSCountyCode', 'CensusTractIdentifier',
                                      'CensusTractMinorityRatioPercent',
                                      'LocalAreaMedianIncomeAmount',
                                      'CensusTractMedFamIncomeAmount',
                                      'TotalMonthlyIncomeAmount',
                                      'NoteAmount',
                                      'HousingExpenseRatioPercent',
                                      'TotalDebtExpenseRatioPercent',
                                      'LoanAcquisitionActualUPBAmt',
                                      'LTVRatioPercent',
                                      'Borrower1CreditScoreValue',
                                      'PMICoveragePercent',
                                      'Borrower1AgeAtApplicationYears',
                                      'Borrower1Race1Type',
                                      'Borrower1EthnicityType',
                                      'Borrower1GenderType',
                                      'PropertyType',
                                      'MortgageType']]

fin_char_data.head()

fin_char_data.shape

fin_char_data.hist(figsize=(10, 10), layout=(5, 4), sharex=False, sharey=False)
plt.tight_layout()
plt.show()

borrower_profile_data = first_time_buyers_v3[['Bank', 'FIPSStateNumericCode',
                                              'FIPSCountyCode', 'CensusTractIdentifier',
                                              'CensusTractMinorityRatioPercent', 'LocalAreaMedianIncomeAmount',
                                              'CensusTractMedFamIncomeAmount', 'TotalMonthlyIncomeAmount', 'Borrower1CreditScoreValue',
                                              'HousingExpenseRatioPercent', 'TotalDebtExpenseRatioPercent',
                                              'LoanPurposeType', 'ScheduledTotalPaymentCount',
                                              'LoanAcquisitionActualUPBAmt',
                                              'LTVRatioPercent',
                                              'LoanAmortizationMaxTermMonths',
                                              'Borrower1AgeAtApplicationYears',
                                              'Borrower1Race1Type', 'Borrower1EthnicityType', 'Borrower1GenderType',
                                              'EmploymentBorrowerSelfEmployed', 'ProductCategoryName',
                                              'PropertyUnitCount']]

borrower_profile_data.head()

borrower_profile_data.shape

"""# **Data Visualisation**

**1. Histograms Plots:**
- Income
- LTVRatioPercent
- LoanAcquisitionActualUPBAmt
"""

# prompt: Histograms Plots:
# - Income
# - LTVRatioPercent
# - LoanAcquisitionActualUPBAmt

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
sns.histplot(fin_char_data['TotalMonthlyIncomeAmount'], kde=True)
plt.title('Distribution of Total Monthly Income')

plt.subplot(1, 3, 2)
sns.histplot(fin_char_data['LTVRatioPercent'], kde=True)
plt.title('Distribution of LTV Ratio Percent')

plt.subplot(1, 3, 3)
sns.histplot(fin_char_data['LoanAcquisitionActualUPBAmt'], kde=True)
plt.title('Distribution of Loan Acquisition Actual UPB Amount')

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(fin_char_data['HousingExpenseRatioPercent'], kde = True)
plt.title('Distribution of Housing Expense Ratio Percent')

plt.subplot(1, 2, 2)
sns.histplot(fin_char_data['TotalDebtExpenseRatioPercent'], kde = True)
plt.title('Distribution of Total Debt Expense Ratio Percent')

plt.tight_layout()
plt.show()

"""According to the histograms:
- The Total Monthly Income in the dataset has right-skewed distribution.
- The LTV Ratio Percent in the datset has bimodel distribution.
- The Loan Acquisition actual UPB Amount is right-skewed distribution.
- The Housing Expense Ratio Percent in the data has a right-skewed distribution.
- The Debt Expenses Ratio Percent in the dataset has a right-skewed distribution.

2. Geopandas: Using the FIPSStateNumericCode and FIPSCountyCode to create maps of the following variable:
- LTVRatioPercent (percentage of the LVT Ratio)
- CensusTractMinorityRatioPercent
- LocalAreaMedianIncomeAmount
- CensusTractMedFamIncomeAmount
"""

# shapefiles with corresponding FIPS codes.
!wget -q https://www2.census.gov/geo/tiger/TIGER2024/COUNTY/tl_2024_us_county.zip
!unzip -o -q tl_2024_us_county.zip -d /content/

try:
  counties = gpd.read_file('/content/tl_2024_us_county.shp')
except FileNotFoundError:
  print("Error: Shapefile not found. Please provide the correct path.")
  exit()

# Merge your data with the shapefile based on FIPS codes
counties['STATEFP'] = counties['STATEFP'].astype(int)
counties['COUNTYFP'] = counties['COUNTYFP'].astype(int)

merged_data = counties.merge(fin_char_data,
                             left_on=['STATEFP', 'COUNTYFP'],
                             right_on=['FIPSStateNumericCode', 'FIPSCountyCode'],
                             how='inner')


variables_to_map = ['LTVRatioPercent', 'TotalDebtExpenseRatioPercent', 'CensusTractMinorityRatioPercent',
                    'LocalAreaMedianIncomeAmount', 'CensusTractMedFamIncomeAmount']

for variable in variables_to_map:
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    merged_data.plot(column=variable, cmap='viridis', linewidth=0.5, ax=ax, edgecolor='0.8', legend=True)
    plt.title(f'Map of {variable}')
    plt.show()

"""#**Feature Engineering**

- target variable creation
- normalisaton of the algorithms (log-normal)

##Target Variable Creation

I will construct the target variable to feature those who are at risk to mortgage deliquency, as there is no variable that shows whether the borrower has defualted. Therefore, I will explore the variables that are best to represent high risk of mortgage deliquency based on this information:


- `Higher HousingExpenseRatioPercent` or `TotalDebtExpenseRatioPercent` could imply higher risk of delinquency.
Example: Flag borrowers with `TotalDebtExpenseRatioPercent` > 43% (a common threshold for loan approval) as "at risk."

- Predict Default Probability:
If you have data indicating whether a loan was fully repaid, in default, or charged off, this could be used as a proxy for mortgage delinquency. (However, I lack the information to identify which variable is best in the original dataset).

- Credit Score Thresholds: Use low `Borrower1CreditScoreValue` as a proxy for high delinquency risk (e.g., below 620 is often considered subprime).
Credit Scores are separated into a range:  1=<620, 2=620  < 660, 3=660 < 700, 4=700 < 760, 5=760 or greater,  9 = Missing or Not Applicable
"""

fin_char_data_v2 = fin_char_data.copy()

max_debt_expense_ratio = fin_char_data_v2['TotalDebtExpenseRatioPercent'].max()
print(f"The maximum value for TotalDebtExpenseRatioPercent is: {max_debt_expense_ratio}")

count_debt_expense_greater_than_43 = fin_char_data_v2[fin_char_data_v2['TotalDebtExpenseRatioPercent'] > 43].shape[0]
print(f"Number of rows with TotalDebtExpenseRatioPercentage > 43: {count_debt_expense_greater_than_43}")

count_debt_expense_less_than_43 = fin_char_data_v2[fin_char_data_v2['TotalDebtExpenseRatioPercent'] <= 43].shape[0]
print(f"Number of rows with TotalDebtExpenseRatioPercentage <= 43: {count_debt_expense_less_than_43}")

fin_char_data_v2['HighRiskofMortgageDeliquency'] = fin_char_data_v2['TotalDebtExpenseRatioPercent'].apply(lambda x:1 if x>43 else 0)

plt.figure(figsize=(8, 8))
plt.title('Distribution of High Risk of Mortgage Delinquency')
sns.countplot(x='HighRiskofMortgageDeliquency', data=fin_char_data_v2)
plt.show()

"""We can observe that:
- Number of borrowers that are at high risk of mortgage deliquency: 2268
- Number of borrowers that are at high risk of mortgage deliquency: 18716

### **Data Visualisation after the Creation of the Target Variable**

**Investigate those who are more likely to at risk of mortgage deliquency rate by race `Borrower1Race1Type` , gender `Borrower1GenderType` and by ethnicity `Borrower1EthnicityType` [white non-hispanics and hispanics]**

Context of each variable (borrower's characteristcs'):
- `Borrower1Race1Type`: Numeric code indicating the race of the Borrower.  1=American Indian or Alaska Native, 2=Asian, 3=Black or African American, 4=Native Hawaiian or other Pacific Islander, 5=White, 6=Information not provided by Borrower, 7=Not Applicable (First or primary borrower is an institution, corporation or partnership)
- `Borrower1GenderType`: Numeric code indicating the sex of the first or primary borrower.  1=Male, 2=Female, 3=Information not provided by borrower, 4=Not Applicable (First or primary borrower is an institution, corporation or partnership), 6=Borrower selected both male and female
- `Borrower1EthnicityType`: 1=Hispanic or Latino; 2=Not Hispanic or Latino; 3=Information not provided; 4=Not applicable (First or primary borrower is an institution, corporation or partnership)
"""

# Create the barplots
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.countplot(x='Borrower1Race1Type', hue='HighRiskofMortgageDeliquency', data=fin_char_data_v2)
plt.title('Risk of Mortgage Delinquency by Race')

plt.subplot(1, 3, 2) # 1 row, 3 columns, 2nd subplot
sns.countplot(x='Borrower1GenderType', hue='HighRiskofMortgageDeliquency', data=fin_char_data_v2)
plt.title('Risk of Mortgage Delinquency by Gender')

plt.subplot(1, 3, 3)
sns.countplot(x='Borrower1EthnicityType', hue='HighRiskofMortgageDeliquency', data=fin_char_data_v2)
plt.title('Risk of Mortgage Delinquency by Ethnicity')

plt.tight_layout()
plt.show()

"""**Borrower1Race1Type**

This chart shows how different racial groups experience mortgage delinquency risk:

- Race 5 (White):

  Dominates the dataset, with most borrowers being in the low-risk category (blue bar).
  A relatively small proportion falls into the high-risk category (orange bar).

- Race 3 (Black or African American):
  Higher relative proportion of borrowers are in the high-risk category compared to other races, suggesting greater vulnerability to mortgage delinquency.

- Other Races (1, 2, 4, 6, 7):
  Smaller representation in the dataset.
  Proportions of high-risk borrowers vary but are less prominent due to the smaller sample sizes.

Key Insight: White borrowers are overrepresented in the dataset and largely low-risk. Black or African American borrowers show a higher delinquency risk, potentially pointing to systemic disparities in income, lending policies, or financial stability.

**Borrower1GenderType**

This chart reflects the delinquency risk based on gender:

- Gender 1 (Male):
Most borrowers are male, and the majority are in the low-risk category (blue bar).
A smaller proportion is high-risk (orange bar).

- Gender 2 (Female):
Fewer borrowers than males, but a higher proportion of females fall into the high-risk category, suggesting that female borrowers may face greater financial challenges.

- Other Categories (3, 4, 6):
Limited representation in the dataset, making it harder to infer patterns.

Key Insight: Female borrowers seem to have a slightly higher risk of delinquency relative to males, which could be linked to gender pay gaps or other socioeconomic factors.

**Borrower1EthnicityType**
This chart focuses on mortgage delinquency risk by ethnicity:

- Ethnicity 2 (Not Hispanic or Latino):
Largest group in the dataset, with most borrowers in the low-risk category (blue bar).
A small proportion is high-risk (orange bar).

- Ethnicity 1 (Hispanic or Latino):
Smaller group overall but has a higher proportion of high-risk borrowers compared to Ethnicity 2.

- Other Categories (3, 4):
Limited data, so patterns are harder to interpret.

Key Insight: Hispanic or Latino borrowers show a higher proportion of delinquency risk, potentially reflecting income disparities, lending practices, or other financial barriers.


"""

# Pie charts for HighRiskofMortgageDeliquency == 0
no_risk_data = fin_char_data_v2[fin_char_data_v2['HighRiskofMortgageDeliquency'] == 0]

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
no_risk_data['Borrower1Race1Type'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Race Distribution (No Risk)')

plt.subplot(1, 3, 2)
no_risk_data['Borrower1GenderType'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Gender Distribution (No Risk)')

plt.subplot(1, 3, 3)
no_risk_data['Borrower1EthnicityType'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Ethnicity Distribution (No Risk)')

plt.tight_layout()
plt.show()

# Pie charts for HighRiskofMortgageDeliquency == 1
high_risk_data = fin_char_data_v2[fin_char_data_v2['HighRiskofMortgageDeliquency'] == 1]

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
high_risk_data['Borrower1Race1Type'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Race Distribution (High Risk)')

plt.subplot(1, 3, 2)
high_risk_data['Borrower1GenderType'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Gender Distribution (High Risk)')

plt.subplot(1, 3, 3)
high_risk_data['Borrower1EthnicityType'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Ethnicity Distribution (High Risk)')

plt.tight_layout()
plt.show()

# Filter for Hispanic White
hispanic_white = fin_char_data_v2[(fin_char_data_v2['Borrower1Race1Type'] == 5) & (fin_char_data_v2['Borrower1EthnicityType'] == 1)]

# Filter for Non-Hispanic White
non_hispanic_white = fin_char_data_v2[(fin_char_data_v2['Borrower1Race1Type'] == 5) & (fin_char_data_v2['Borrower1EthnicityType'] == 2)]

plt.figure(figsize=(15, 5))
# Create the first barplot (Hispanic White)

def addlabels(x,y):
    for i in range(len(y)):
        plt.text(y.index[i], y.values[i]//2, y.values[i], ha = 'center')

plt.subplot(1, 2, 1)
sns.countplot(x='HighRiskofMortgageDeliquency', data=hispanic_white)
addlabels(hispanic_white['HighRiskofMortgageDeliquency'], hispanic_white['HighRiskofMortgageDeliquency'].value_counts())
plt.title('Risk of Mortgage Delinquency for Hispanic White Borrowers')
plt.xlabel('High Risk of Mortgage Delinquency')
plt.ylabel('Count')

# Create the second barplot (Non-Hispanic White)

plt.subplot(1, 2, 2)
sns.countplot(x='HighRiskofMortgageDeliquency', data=non_hispanic_white)
addlabels(non_hispanic_white['HighRiskofMortgageDeliquency'], non_hispanic_white['HighRiskofMortgageDeliquency'].value_counts())
plt.title('Risk of Mortgage Delinquency for Non-Hispanic White Borrowers')
plt.xlabel('High Risk of Mortgage Delinquency')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

# Filter for White borrowers
white_borrowers = fin_char_data_v2[fin_char_data_v2['Borrower1Race1Type'] == 5]

# Filter by ethnicity
hispanic_white_borrowers = white_borrowers[white_borrowers['Borrower1EthnicityType'] == 1]
non_hispanic_white_borrowers = white_borrowers[white_borrowers['Borrower1EthnicityType'] == 2]

# Calculate the counts for each group and risk level
hispanic_high_risk = hispanic_white_borrowers['HighRiskofMortgageDeliquency'].sum()
hispanic_low_risk = len(hispanic_white_borrowers) - hispanic_high_risk
non_hispanic_high_risk = non_hispanic_white_borrowers['HighRiskofMortgageDeliquency'].sum()
non_hispanic_low_risk = len(non_hispanic_white_borrowers) - non_hispanic_high_risk

labels = ['Hispanic White High Risk', 'Hispanic White Low Risk', 'Non-Hispanic White High Risk', 'Non-Hispanic White Low Risk']
sizes = [hispanic_high_risk, hispanic_low_risk, non_hispanic_high_risk, non_hispanic_low_risk]
explode = (0.1, 0, 0.1, 0)  # explode the 1st and 3rd slices

fig, ax = plt.subplots()
ax.pie(sizes, explode = explode, labels=labels, autopct='%1.1f%%', startangle=90)
ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

"""- Non-Hispanic white borrowers dominate the low-risk category (84.5%), suggesting they generally have better financial characteristics or access to more favorable mortgage terms.

- Hispanic white borrowers have a much smaller share of low-risk and high-risk categories, potentially reflecting disparities in access to credit, socioeconomic differences, or other structural factors.

##Log Transformation (Normalisation)
"""

fin_char_data_v3 = fin_char_data_v2.copy() #copy to have a dataset that has been normalised

columns = ['TotalMonthlyIncomeAmount', 'LTVRatioPercent', 'LoanAcquisitionActualUPBAmt', 'HousingExpenseRatioPercent', 'TotalDebtExpenseRatioPercent']
for i in columns:
  fin_char_data_v3[i] = fin_char_data_v3[i].map(lambda x: np.log(x) if x!= 0 else 0)

fin_char_data_v3.shape

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
sns.histplot(fin_char_data_v3['TotalMonthlyIncomeAmount'], kde=True)
plt.title('Log Distribution of Total Monthly Income')

plt.subplot(1, 3, 2)
sns.histplot(fin_char_data_v3['LTVRatioPercent'], kde=True)
plt.title('Log Distribution of LTV Ratio Percent')

plt.subplot(1, 3, 3)
sns.histplot(fin_char_data_v3['LoanAcquisitionActualUPBAmt'], kde=True)
plt.title('Log Distribution of Loan Acquisition Actual UPB Amount')

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(fin_char_data_v3['HousingExpenseRatioPercent'], kde = True)
plt.title('Distribution of Housing Expense Ratio Percent')

plt.subplot(1, 2, 2)
sns.histplot(fin_char_data_v3['TotalDebtExpenseRatioPercent'], kde = True)
plt.title('Distribution of Total Debt Expense Ratio Percent')

plt.tight_layout()
plt.show()

"""According to the histograms:
- The Total Monthly Income in the dataset now follows a normal distribution.
- The LTV Ratio Percent in the datset now follows a left skewed distribution.
- The Loan Acquisition actual UPB Amount now follows a normal distribution.

Therefore, we need to use an outlier detector to understand the distribution of the selected columns using boxplots.
"""

plt.figure(figsize=(12, 6))

plt.subplot(2, 3, 1)
sns.boxplot(y=fin_char_data_v3['TotalMonthlyIncomeAmount'])
plt.title('Log Total Monthly Income')

plt.subplot(2, 3, 2)
sns.boxplot(y=fin_char_data_v3['LTVRatioPercent'])
plt.title('Log LTV Ratio Percent')

plt.subplot(2, 3, 3)
sns.boxplot(y=fin_char_data_v3['LoanAcquisitionActualUPBAmt'])
plt.title('Log Loan Acquisition Actual UPB Amount')

plt.subplot(2, 3, 4)
sns.boxplot(y=fin_char_data_v3['HousingExpenseRatioPercent'])
plt.title('Log Housing Expense Ratio Percent')

plt.subplot(2, 3, 5)
sns.boxplot(y=fin_char_data_v3['TotalDebtExpenseRatioPercent'])
plt.title('Log Total Debt Expense Ratio Percent')


plt.tight_layout()
plt.show()

"""We can observe that there are a lot of outliers for the selected columns: 'TotalMonthlyIncomeAmount', 'LTVRatioPercent', 'LoanAcquisitionActualUPBAmt', 'HousingExpenseRatioPercent', 'TotalDebtExpenseRatioPercent'.

Therefore, we have to use PCA models to help reduce the dimensionality for machine learning methods to do multivariate analysis.

#**PCA analysis**
- train the set with the normalisation (fin_char_data_v3)
- train the set without normalisation (fin_char_data_v2)
"""

# train-test split the data for fin_char_data_v2 to 80%-20% respectively by rows
train_data_v2, test_data_v2 = train_test_split(fin_char_data_v2, test_size=0.2, random_state=42) # 80% train, 20% test

print("Training data shape:", train_data_v2.shape)
print("Testing data shape:", test_data_v2.shape)

# train-test split the data for fin_char_data_v3 to 80%-20% respectively by rows
train_data_v3, test_data_v3 = train_test_split(fin_char_data_v3, test_size=0.2, random_state=42) # 80% train, 20% test

print("Training data shape:", train_data_v3.shape)
print("Testing data shape:", test_data_v3.shape)

"""Eliminate the census information and TotalDebtExpenseRatioPercent' (as, it closely related 'HighRiskofMortgageDeliquency') to conduct PCA:

"""

fin_char_data_without_census_v3 = fin_char_data_v3[['TotalMonthlyIncomeAmount',
                                      'NoteAmount',
                                      'HousingExpenseRatioPercent',
                                      'LoanAcquisitionActualUPBAmt',
                                      'LTVRatioPercent',
                                      'Borrower1CreditScoreValue',
                                      'PMICoveragePercent',
                                      'Borrower1AgeAtApplicationYears',
                                      'Borrower1Race1Type',
                                      'Borrower1EthnicityType',
                                      'Borrower1GenderType',
                                      'PropertyType',
                                      'MortgageType',
                                      'HighRiskofMortgageDeliquency']]

"""Convert the 'PropertyType' into numerical categorical orders to scale it:

- 1 - PT01=Single family detached;
- 2 - PT02=Deminimus PUD;
- 3 - PT03=Single family attached;
- 4 - PT04=Two family; PT05=Townhouse;
- 5 - PT06=Low-rise condo;
- 6 - PT07=PUD; PT08=Duplex;
- 7 - PT09=Three family;
- 8 - PT10=Four family;
- 9 - PT11=Hi-res condo;
- 10 - PT12=Manufactured home not chattel;
- 11- PT13=Manufactured home chattel;
- 12 PT14=Five plus multifamily
"""

# Create a mapping dictionary for PropertyType
property_type_mapping = {
    'PT01': 1, 'PT02': 2, 'PT03': 3, 'PT04': 4, 'PT05': 4,
    'PT06': 5, 'PT07': 6, 'PT08': 6, 'PT09': 7, 'PT10': 8,
    'PT11': 9, 'PT12': 10, 'PT13': 11, 'PT14': 12
}

# Apply the mapping to the 'PropertyType' column
fin_char_data_without_census_v3['PropertyType'] = fin_char_data_without_census_v3['PropertyType'].map(property_type_mapping)

"""Then, PCA Analysis is constructed with the normalised dataset:"""

#Standard scaling
#PCA requires scaling/normalization of the data to work properly
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

#Define X and y from the dataset
X_v3 = fin_char_data_without_census_v3.drop('HighRiskofMortgageDeliquency',axis=1)
y_v3 = fin_char_data_without_census_v3['HighRiskofMortgageDeliquency']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_v3, y_v3, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_v3_scaled = scaler.fit_transform(X_train)
X_test_v3_scaled = scaler.transform(X_test)

# Get column names from the original DataFrame
column_names = X_v3.columns # Get column names before scaling

# Create a DataFrame with the scaled trained data and original column names
dfx3 = pd.DataFrame(data=X_train_v3_scaled, columns=column_names)

#PCA class import and analysis
from sklearn.decomposition import PCA

#n_components (int, float or ‘mle’, default=None)
#Number of components to keep. if n_components is not set all components are kept:

pca = PCA(n_components=None)
dfx3_pca = pca.fit(dfx3)

#Now, let's visualize the PCA results
plt.figure(figsize=(10,6))
plt.scatter(x=[i+1 for i in range(len(dfx3_pca.explained_variance_ratio_))],
            y=dfx3_pca.explained_variance_ratio_,
           s=200, alpha=0.75,c='blue',edgecolor='k')
plt.grid(True)
plt.title("Explained variance ratio of the \nfitted principal component vector\n",fontsize=25)
plt.xlabel("Principal components",fontsize=15)
plt.xticks([i+1 for i in range(len(dfx3_pca.explained_variance_ratio_))],fontsize=15)
plt.yticks(fontsize=15)
plt.ylabel("Explained variance ratio",fontsize=15)
plt.show()

"""To explain 95% variance in our normalised fin_char_data_without_census_v3 dataset, we reduce the dimensionality from 13 to 10 principal components.




"""

# Apply PCA to reduce dimensionality to explain 95% of variance.
pca = PCA(n_components=0.95) # Use 0.95 to automatically select components
X_train_v3_pca = pca.fit_transform(X_train_v3_scaled)
X_test_v3_pca = pca.transform(X_test_v3_scaled)

"""Calculate the PCA loadings - it will come in handy when examining the logistic regression, as it shows which variables that best represent each principal components."""

# Calculate PCA loadings
loadings = pd.DataFrame(pca.components_.T, index=X_v3.columns,
                        columns=[f"PC{i+1}" for i in range(pca.n_components_)])
print(loadings)

"""**Revise this evaluation**

| Principal Components (PC) | Most Contributing Variable | Loading Value | Interpretation |
|:-----|:------:|:------:|-----:|
|PC1 | LoanAcquisitionActualUPBAmt | 0.541336 | Financial size of the Loan |
|PC2 | HousingExpenseRatioPercent | 0.545058 | Housing cost strain |
|PC3 | PMICoveragePercent  | 0.534124 | Role of private mortgage insurance|
|PC4 | Borrower1GenderType | 0.596926  | Gender-related effects|
|PC5 | Borrower1CreditScoreValue | 0.585705 | Borrower-creditworthiness|
|PC6| Borrower1AgeAtApplicationYears| 0.633834 | Age effects|
|PC7| LTVRatioPercent | 0.177803 | Loan-to-Value equity risks|
|PC8| Borrower1EthnicityType | 0.336180 | Socio-cultural factors|
|PC9| PropertyType | 0.772302| Property (real estate) type effects|
|PC10| Borrower1EthnicityType| 0.583556 | Socio-cultural factors|

# **Data Modelling**

After training the set and doing the principal component analysis (to explain 95% variance of our dataset), the following methods will be used to multivariate analysis:
- Supervised machine learning models (Decision Trees Classifion Tree and Random Forests)  
- Logistic regression models
- ROC
- AUC

Then, compare the models and see which one is the best. Also, i want to investigate if race is factor that will lead to mortgage deliquency rate.

##Decision Tree Classifier Tree
"""

# from sklearn.tree import DecisionTreeClassifier, plot_tree
# from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

dt_classifier = DecisionTreeClassifier(random_state=42)  # Consider hyperparameter tuning
dt_classifier.fit(X_train_v3_pca, y_train)

clf = DecisionTreeClassifier()
clf = clf.fit(X_train_v3_pca, y_train)
plot_tree(clf)

# Make predictions
y_pred = dt_classifier.predict(X_test_v3_pca)
y_pred_proba = dt_classifier.predict_proba(X_test_v3_pca)[:,1]


# Evaluate the model
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print("AUC Score:", roc_auc_score(y_test, y_pred_proba))

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
plt.plot(fpr, tpr, label=f'Decision Tree (AUC = {roc_auc_score(y_test, y_pred_proba):.2f})')
plt.plot([0, 1], [0, 1], 'k--') # diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

from sklearn import tree
plt.figure(figsize=(20,20))
features = X_v3.columns
classes = ['Low Risk of Delinquency','High Risk of Delinquency']
tree.plot_tree(clf,feature_names=features,class_names=classes,filled=True)
plt.show()

"""## Pruning the Decision Tree: Classifier Tree

**Credit:** https://www.kaggle.com/code/arunmohan003/pruning-decision-trees-tutorial
"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
params = {'max_depth': [2,4,6,8,10,12],
         'min_samples_split': [2,3,4],
         'min_samples_leaf': [1,2]}

clf = tree.DecisionTreeClassifier()
gcv = GridSearchCV(estimator=clf,param_grid=params)
gcv.fit(X_train_v3_pca,y_train)

def plot_confusionmatrix(y_train_pred,y_train,dom):
    print(f'{dom} Confusion matrix')
    cf = confusion_matrix(y_train_pred,y_train)
    sns.heatmap(cf,annot=True,yticklabels=classes
               ,xticklabels=classes,cmap='Blues', fmt='g')
    plt.tight_layout()
    plt.show()

model = gcv.best_estimator_
model.fit(X_train_v3_pca,y_train)
y_train_pred = model.predict(X_train_v3_pca)
y_test_pred = model.predict(X_test_v3_pca)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Test score {accuracy_score(y_test_pred,y_test)}')
plot_confusionmatrix(y_train_pred,y_train,dom='Train')
plot_confusionmatrix(y_test_pred,y_test,dom='Test')

"""Decision trees can easily overfit. One way to avoid it is to limit the growth of trees by setting constrains. We can limit parameters like max_depth , min_samples etc. But a most effective way is to use post pruning methods like cost complexity pruning. This helps to improve test accuracy and get a better model.

Cost complexity pruning is all about finding the right parameter for alpha.We will get the alpha values for this tree and will check the accuracy with the pruned trees.
"""

path = clf.cost_complexity_pruning_path(X_train_v3_pca, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
print(ccp_alphas)

# For each alpha we will append our model to a list
clfs = []
for ccp_alpha in ccp_alphas:
    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    clf.fit(X_train_v3_pca, y_train)
    clfs.append(clf)

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]
node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
plt.scatter(ccp_alphas,node_counts)
plt.scatter(ccp_alphas,depth)
plt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle="steps-post")
plt.plot(ccp_alphas,depth,label='depth',drawstyle="steps-post")
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score,confusion_matrix
train_acc = []
test_acc = []
for c in clfs:
    y_train_pred = c.predict(X_train_v3_pca)
    y_test_pred = c.predict(X_test_v3_pca)
    train_acc.append(accuracy_score(y_train_pred,y_train))
    test_acc.append(accuracy_score(y_test_pred,y_test))

plt.scatter(ccp_alphas,train_acc)
plt.scatter(ccp_alphas,test_acc)
plt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle="steps-post")
plt.plot(ccp_alphas,test_acc,label='test_accuracy',drawstyle="steps-post")
plt.legend()
plt.title('Accuracy vs alpha')
plt.show()

"""We choose 0.001"""

clf_ = tree.DecisionTreeClassifier(random_state=0,ccp_alpha=0.001)
clf_.fit(X_train_v3_pca,y_train)
y_train_pred = clf_.predict(X_train_v3_pca)
y_test_pred = clf_.predict(X_test_v3_pca)

print(f'Train score {accuracy_score(y_train_pred,y_train)}')
print(f'Test score {accuracy_score(y_test_pred,y_test)}')
plot_confusionmatrix(y_train_pred,y_train,dom='Train')
plot_confusionmatrix(y_test_pred,y_test,dom='Test')

plt.figure(figsize=(20,20))
features = X_v3.columns
classes = ['Low Risk of Delinquency','High Risk of Delinquency']
tree.plot_tree(clf_,feature_names=features,class_names=classes,filled=True)
plt.show()

# Assuming clf_ and X_test_v3_pca are defined from the previous code
y_pred_proba = clf_.predict_proba(X_test_v3_pca)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""##Random Forests

Random Forest Importance Features:
Want to investigate which characteristic is important with the pca X_train_v3_pca = pca.fit_transform(X_train_v3_scaled)
X_test_v3_pca = pca.transform(X_test_v3_scaled)
"""

from sklearn.ensemble import RandomForestClassifier

# Initialize and train the RandomForestClassifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_v3_pca, y_train)

# Get feature importances
importances = rf_classifier.feature_importances_

# a DataFrame to display importances
feature_importances = pd.DataFrame({'Feature': range(1,len(importances)+1), 'Importance': importances})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Print or visualize feature importances
print(feature_importances)


plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Random Forest Feature Importances (With PCA)')
plt.show()

"""Random Forest Importance Features: Want to investigate which characteristic is important without the pca #Define X and y from the dataset
X_v3 = fin_char_data_without_census_v3.drop('HighRiskofMortgageDeliquency',axis=1)
y_v3 = fin_char_data_without_census_v3['HighRiskofMortgageDeliquency']
X_train, X_test, y_train, y_test = train_test_split(X_v3, y_v3, test_size=0.2, random_state=42)
"""

# Initialize and train the RandomForestClassifier
rf_classifier_no_pca = RandomForestClassifier(random_state=42)
rf_classifier_no_pca.fit(X_train, y_train)

# Get feature importances
importances = rf_classifier_no_pca.feature_importances_

# a DataFrame to display importances
feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Print or visualize feature importances
print(feature_importances)


plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Random Forest Feature Importances (Without PCA)')
plt.show()

# trained RandomForestClassifier model (with PCA)
y_pred_proba_rf_pca = rf_classifier.predict_proba(X_test_v3_pca)[:, 1]
fpr_rf_pca, tpr_rf_pca, thresholds_rf_pca = roc_curve(y_test, y_pred_proba_rf_pca)
auc_rf_pca = roc_auc_score(y_test, y_pred_proba_rf_pca)

#  trained RandomForestClassifier model (without PCA) - you need to retrain it without PCA data
y_pred_proba_rf_no_pca = rf_classifier_no_pca.predict_proba(X_test)[:, 1]
fpr_rf_no_pca, tpr_rf_no_pca, thresholds_rf_no_pca = roc_curve(y_test, y_pred_proba_rf_no_pca)
auc_rf_no_pca = roc_auc_score(y_test, y_pred_proba_rf_no_pca)


plt.figure(figsize=(10, 6))
plt.plot(fpr_rf_pca, tpr_rf_pca, label=f'Random Forest (With PCA) (AUC = {auc_rf_pca:.2f})')
plt.plot(fpr_rf_no_pca, tpr_rf_no_pca, label=f'Random Forest (Without PCA) (AUC = {auc_rf_no_pca:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Random Forest Models')
plt.legend(loc='lower right')
plt.show()

print(f"Random Forest (With PCA) AUC: {auc_rf_pca}")
print(f"Random Forest (Without PCA) AUC: {auc_rf_no_pca}")

y_pred_no_pca = rf_classifier_no_pca.predict(X_test)

# Create the confusion matrix
cm_no_pca = confusion_matrix(y_test, y_pred_no_pca)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_no_pca, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Low Risk', 'High Risk'], yticklabels=['Low Risk', 'High Risk'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Random Forest without PCA)")
plt.show()

"""##Logistic Regression"""

from sklearn.linear_model import LogisticRegression
# Import statsmodels
import statsmodels.api as sm

"""Computing the Logistic Regression with the PCA"""

#X_train_v3_pca, y_train
X_train_pa_sm = sm.add_constant(X_train_v3_pca)
logm2 = sm.GLM(y_train, X_train_pa_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

"""The regression results involves a Principal Component Analysis (PCA) model where the original variables are reduced into components (x1, x2, ..., x10).

Key Model Information:
  - Dependent Variable: HighRiskofMortgageDelinquency (binary outcome).
  - Model Type: Binomial with a Logit (logistic) link function.
  - Sample Size: 16,904 observations.
  - Pseudo R-squared (CS): 0.1233 indicates that about 12.33% of the variability in the dependent variable is explained by the principal components.
  - Log-Likelihood: -4602.5, slightly better than the earlier model, suggesting PCA might provide some improvement in fit.

Here's a summary of the regression analysis:
- **`x1` (First Principal Component)**
Positive coefficient (0.1550): A higher score on this component increases the risk.
P-value < 0.001: Highly significant.
- **`x2` (Second Principal Component)**
Positive coefficient (0.9518): A strong positive association with delinquency risk.
P-value < 0.001: Highly significant.
- **`x3` (Third Principal Component)**
Positive coefficient (0.5606): Significant positive effect on risk.
P-value < 0.001: Highly significant.
- **`x4` (Fourth Principal Component)**
Positive coefficient (0.4733): Contributes significantly to the likelihood of delinquency.
P-value < 0.001: Highly significant.
- **`x5` (Fifth Principal Component)**
Negative coefficient (-0.0928): A higher score on this component reduces the risk of delinquency.
P-value = 0.001: Significant.
- **`x7` (Seventh Principal Component)**
Positive coefficient (0.3359): Increases delinquency risk.
P-value < 0.001: Highly significant.
- **`x10` (Tenth Principal Component)**
Positive coefficient (0.1364): Smaller effect but still significant.
P-value = 0.001: Significant.
- **Non-Significant Components:**
`x6` (P-value = 0.692) and `x9` (P-value = 0.387) do not significantly contribute to the model. Their coefficients are close to zero, suggesting minimal impact on the risk of delinquency.

**Summary:**
The PCA model reduces the dimensionality of the original dataset, and most of the significant principal components (e.g., `x1`, `x2`, `x3`, `x4`) are positively associated with mortgage delinquency risk.
The PCA regression provides a similar overall fit (Pseudo R-squared = 0.1233) compared to the first model but simplifies the interpretation by focusing on fewer, aggregated components rather than individual variables.
Components like `x5` and `x7` demonstrate nuanced relationships (e.g., both increasing and reducing risk), showing the power of PCA in capturing underlying patterns.

Computing the Logistic Regression without the PCA
"""

#X_train, y_train
X_train_sm = sm.add_constant(X_train)
logm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

"""
The table shows the results of a **Generalized Linear Model (GLM) regression analysis.** Here’s a summary of the key insights:

**Key Information**
- Dependent Variable: HighRiskofMortgageDelinquency (indicating the probability of being high risk for mortgage delinquency, modeled as a binary outcome).
- Model Type: Binomial (appropriate for binary classification) with a Logit (logistic) link function.
- Sample Size: 16,904 observations were used.
- Pseudo R-squared (CS): 0.1312 indicates the model explains approximately 13.12% of the variability in the dependent variable. While low, this is not uncommon for logistic regression in social sciences and business applications.


**Interpreting Key Predictors:**
The coefficients (coef) represent the effect of each predictor on the log-odds of being classified as high risk. Here's an overview of the significant predictors:

- **`TotalMonthlyIncomeAmount`**

  Positive coefficient (0.7671): Higher monthly income increases the log-odds of being high risk.
  P-value < 0.001: This predictor is statistically significant.

- **`HousingExpenseRatioPercent`**

  Large positive coefficient (4.0847): A higher housing expense ratio significantly increases the risk of delinquency.
  P-value < 0.001: Strong evidence for significance.
- **`LTVRatioPercent`** (Loan-to-Value Ratio)

  Positive coefficient (1.8346): Higher LTV ratios increase delinquency risk, suggesting more leveraged loans are riskier.
  P-value < 0.001: Significant.
- **`Borrower1CreditScoreValue`**

  Negative coefficient (-0.3763): Higher credit scores decrease the risk of delinquency.
  P-value < 0.001: Very significant.
- **`Borrower1AgeAtApplicationYears`**

  Positive coefficient (0.0120): Older borrowers have a slightly higher risk, though the effect is small.
  P-value < 0.001: Significant.
- **`PMICoveragePercent`**

  Negative coefficient (-0.0134): Higher private mortgage insurance (PMI) coverage reduces risk.
  P-value < 0.001: Significant.
- **`Borrower1EthnicityType`**

  Negative coefficient (-0.2883): Certain ethnicities have a lower associated risk.
  P-value < 0.001: Statistically significant.
- **`Borrower1GenderType`**

  Positive coefficient (0.1731): Gender has a small but significant impact on delinquency risk.
  P-value < 0.001: Statistically significant.
- **`PropertyType and MortgageType`**

  Small coefficients, with property type being significant but mortgage type not statistically significant.
- **Non-Significant Predictors:**

  **`NoteAmount`** and **`LoanAcquisitionActualUPBAmt`** have p-values > 0.05, meaning they do not significantly affect the log-odds of being high risk.


**Summary:**

The model identifies key financial factors (like income, housing expense ratio, and credit score) as significant contributors to mortgage delinquency risk.
LTV ratio and PMI coverage are particularly relevant for lenders managing high-risk loans.

Demographic variables (age, ethnicity, and gender) also play a role but may need cautious interpretation due to potential ethical or legal implications."""

logreg = LogisticRegression()
logreg.fit(X_train, y_train)   # Fit the model to your training data
y_pred = logreg.predict(X_test)  # Predict using the logistic regression model
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Predicted Low Risk', 'Predicted High Risk'],
            yticklabels=['Actual Low Risk', 'Actual High Risk'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (Logistic Regression)")
plt.show()

#should I do a VIF model

"""## LightGBM

Test lightGBM with PCA
"""

#build the model
import lightgbm as lgb
clf_lgb_pa = lgb.LGBMClassifier()
clf_lgb_pa.fit(X_train_v3_pca, y_train)

# predict the results
y_pred_pa=clf_lgb_pa.predict(X_test_v3_pca)

# Assuming clf_lgb_pa is your trained LightGBM model
feature_importances = clf_lgb_pa.feature_importances_


feature_importance_df = pd.DataFrame({'Feature': range(1, len(feature_importances) + 1),
                                      'Gain': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Gain', ascending=False)

# Plotting feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Gain'])
plt.xlabel('Gain')
plt.ylabel('Feature (PCA)')
plt.title('LightGBM Feature Importance (PCA)')
plt.show()

# view accuracy
from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred_pa, y_test)
print('LightGBM Model accuracy score with PCA: {0:0.4f}'.format(accuracy_score(y_test, y_pred_pa)))

#Classification report of the lightGBM model with PCA
print(classification_report(y_test, y_pred_pa))

"""Test lightGBM without PCA"""

clf_lgb = lgb.LGBMClassifier()
clf_lgb.fit(X_train, y_train)

# predict the results
y_pred=clf_lgb.predict(X_test)

# Assuming clf_lgb is your trained LightGBM model (without PCA)
feature_importances = clf_lgb.feature_importances_

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({'Feature': X_train.columns,  # Use column names
                                      'Gain': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Gain', ascending=False)

# Plotting feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Gain'])
plt.xlabel('Gain')
plt.ylabel('Feature')
plt.title('LightGBM Feature Importance (Without PCA)')
plt.show()

accuracy=accuracy_score(y_pred, y_test)
print('LightGBM Model accuracy score without PCA: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))

#Classification report of the lightGBM model without PCA
print(classification_report(y_test, y_pred))

# Predict probabilities for LightGBM with PCA
y_pred_proba_lgb_pca = clf_lgb_pa.predict_proba(X_test_v3_pca)[:, 1]
fpr_lgb_pca, tpr_lgb_pca, thresholds_lgb_pca = roc_curve(y_test, y_pred_proba_lgb_pca)
auc_lgb_pca = roc_auc_score(y_test, y_pred_proba_lgb_pca)

# Predict probabilities for LightGBM without PCA
y_pred_proba_lgb = clf_lgb.predict_proba(X_test)[:, 1]
fpr_lgb, tpr_lgb, thresholds_lgb = roc_curve(y_test, y_pred_proba_lgb)
auc_lgb = roc_auc_score(y_test, y_pred_proba_lgb)

# Plot ROC curves
plt.figure(figsize=(10, 6))
plt.plot(fpr_lgb_pca, tpr_lgb_pca, label=f'LightGBM (With PCA) (AUC = {auc_lgb_pca:.2f})')
plt.plot(fpr_lgb, tpr_lgb, label=f'LightGBM (Without PCA) (AUC = {auc_lgb:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for LightGBM Models')
plt.legend(loc='lower right')
plt.show()

print(f"LightGBM (With PCA) AUC: {auc_lgb_pca}")
print(f"LightGBM (Without PCA) AUC: {auc_lgb}")